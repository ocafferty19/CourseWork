{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ba1f21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 14:48:54.641896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3e20e52",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Stanford IMDB dataset - Downloaded from:\n",
    "# https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "df = pd.read_csv('IMDBDataset.csv')\n",
    "# Our own test data - screenshot of data in report\n",
    "oxymoron_df = pd.read_csv('Oxymorons.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b27a2ae5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9980, 57055) (9980,) (10020, 57055) (10020,)\n"
     ]
    }
   ],
   "source": [
    "# Due to our kernel crashing, we had to reduce the test and training size for our model\n",
    "oxymoron_size = 20\n",
    "train_size = 10000\n",
    "test_size = 2000\n",
    "total_size = oxymoron_size + test_size + train_size\n",
    "\n",
    "data = df[0:total_size].to_dict(orient='records')\n",
    "\n",
    "data_text, data_labels = list(zip(*map(lambda d: (d['review'], d['sentiment']), data)))\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "data_vectorized = vectorizer.fit_transform(data_text)\n",
    "\n",
    "data_boolean_labels = np.array(data_labels) == 'positive'\n",
    "data_binary_labels = data_boolean_labels.astype(int)\n",
    "\n",
    "oxymoron_x = data_vectorized[0:oxymoron_size]\n",
    "oxymoron_y = data_binary_labels[0:oxymoron_size]\n",
    "\n",
    "train_vectorized = data_vectorized[oxymoron_size:train_size]\n",
    "test_vectorized = data_vectorized[test_size:total_size]\n",
    "\n",
    "train_y = data_binary_labels[oxymoron_size:train_size]\n",
    "test_y = data_binary_labels[test_size:total_size]\n",
    "\n",
    "print(train_vectorized.shape, train_y.shape, test_vectorized.shape, test_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b062160a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframeToVectorizedData(data):\n",
    "    \n",
    "    data = data.to_dict(orient='records')\n",
    "    \n",
    "    data_text, data_labels = list(zip(*map(lambda d: (d['review'], d['sentiment']), data)))\n",
    "    \n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    data_vectorized = vectorizer.fit_transform(data_text)\n",
    "\n",
    "    data_boolean_labels = np.array(data_labels) == 'positive'\n",
    "    data_binary_labels = data_boolean_labels.astype(int)\n",
    "    \n",
    "    data_y = data_binary_labels\n",
    "    \n",
    "    return data_vectorized, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff008fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNaiveBayes:\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        N, D = x.shape\n",
    "        C = np.max(y) + 1\n",
    "        # one parameter for each feature conditioned on each class\n",
    "        mu, sigma = np.zeros((C,D)), np.zeros((C,D))\n",
    "        Nc = np.zeros(C) # number of instances in class c\n",
    "        # calculate MLE for the mean and std\n",
    "        for c in range(C):\n",
    "            x_c = x[y == c]                \n",
    "            Nc[c] = x_c.shape[0]                     \n",
    "            mu[c,:] = np.mean(x_c,0)                 \n",
    "            sigma[c,:] = np.std(x_c, 0)               \n",
    "        self.mu = mu                                  # C x D\n",
    "        self.sigma = sigma                            # C x D\n",
    "        # Laplace Smoothing\n",
    "        self.pi = (Nc+1)/(N+C)\n",
    "        return self\n",
    "    \n",
    "    def logsumexp(self, Z):                                            \n",
    "        Zmax = np.max(Z,axis=0)[None,:]                              \n",
    "        log_sum_exp = Zmax + np.log(np.sum(np.exp(Z - Zmax), axis=0))\n",
    "        return log_sum_exp\n",
    "    \n",
    "    def predict(self, xt):\n",
    "        Nt, D = xt.shape\n",
    "        log_prior = np.log(self.pi)[:, None]\n",
    "        EPSILON = 1e-8\n",
    "        sigma = self.sigma[:, None, :] + EPSILON  # add epsilon to sigma to avoid division by zero or negative values in the logarithm\n",
    "        log_likelihood = -.5 * np.log(2*np.pi) - np.log(sigma) - .5 * (((xt[None, :, :] - self.mu[:, None, :])/sigma)**2)\n",
    "        log_likelihood = np.sum(log_likelihood, axis=2)\n",
    "        log_posterior = log_prior + log_likelihood\n",
    "        posterior = np.exp(log_posterior - self.logsumexp(log_posterior))\n",
    "        return posterior.T \n",
    "    \n",
    "    def dataframeToVectorizedData(data):\n",
    "        \n",
    "        data = data.to_dict(orient='records')\n",
    "\n",
    "        data_text, data_labels = list(zip(*map(lambda d: (d['review'], d['sentiment']), data)))\n",
    "\n",
    "        vectorizer = CountVectorizer()\n",
    "\n",
    "        data_vectorized = vectorizer.fit_transform(data_text)\n",
    "\n",
    "        data_boolean_labels = np.array(data_labels) == 'positive'\n",
    "        data_binary_labels = data_boolean_labels.astype(int)\n",
    "\n",
    "        data_y = data_binary_labels\n",
    "\n",
    "        return data_vectorized, data_y\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2034b293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.GaussianNaiveBayes at 0x7f97344cb640>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GaussianNaiveBayes()\n",
    "model.fit(train_vectorized.toarray(), train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e58291d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal test data\n",
    "y_prob = model.predict(test_vectorized.toarray())\n",
    "y_pred = np.argmax(y_prob, 1)\n",
    "accuracy = np.sum(y_pred == test_y)/y_pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe983b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8631736526946108"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a86d9ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oxymoron test data\n",
    "oxy_y_prob = model.predict(oxymoron_x.toarray())\n",
    "oxy_y_pred = np.argmax(oxy_y_prob, 1)\n",
    "oxy_accuracy = np.sum(oxy_y_pred == oxymoron_y)/oxy_y_pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f5a293d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oxy_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de52cdb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73697f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
